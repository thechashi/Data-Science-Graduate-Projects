# Comparison between LMs' Summaries'In this project, we focused on natural language processing (NLP) tasks, which involve teaching machines to understand and interpret human language. We explored various NLP tasks, including text analysis, sentiment analysis, language translation, and automatic summarization.Before the introduction of the transformer architecture, recurrent neural networks (RNNs) and long short-term memory (LSTM) networks were widely used in NLP tasks. RNNs are capable of processing sequential data, while LSTMs are specifically designed to handle long-term dependencies. However, these models had limitations in capturing long-range dependencies and handling variable-length input sequences.The transformer architecture revolutionized NLP by introducing self-attention mechanisms, which allowed the model to selectively attend to different parts of the input sequence. Transformers, such as the popular BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) models, became go-to choices for many NLP tasks.T5 is a transformer-based model developed by Google AI Language, consisting of both encoders and decoders. The T5 architecture, with 12 transformer layers in the T5-base variant, revolutionizes text-to-text learning. Unlike models like GPT, T5 predicts erased text chunks, enabling generalization. However, its 512-token size constraint affects summary output length. While T5 generates interesting summaries from regular text, summaries may be smaller, aligned with input. Concerns arise about limited summary size, such as capturing only the initial part of longer reports. Challenges include the tokenizer's difficulty conveying complex medical information, as specific domain-related words are cut up. Attempts to include domain-specific vocabulary led to gibberish summaries, possibly due to overfitting and reduced generalization. Achieving meaningful T5 summaries requires managing input size constraints and handling domain-specific vocabulary effectively.BERT, developed by Google, is a pre-trained deep learning model that uses the transformer architecture. It performs two tasks during pre-training: masked language modeling and next sentence prediction. BERT learns contextualized word embeddings, which capture the meaning of words in context. It can be fine-tuned for various downstream NLP tasks, such as text classification, language translation, and summarization.GPT-2, another transformer-based model, is used for tasks like text generation, summarization, and translation. It consists of multiple layers of transformer blocks and is trained on a large corpus of text data using unsupervised pre-training. GPT-2 can also be fine-tuned for different downstream NLP tasks.For our specific problem of document classification/summarization, we explored different summarization approaches using BERT. We used three variations of BERT, including DistilBART, Facebook BART, and Conversational BART, which had decoders and language model heads suitable for text generation. We compared their performance in summarizing medical texts.By utilizing these NLP models and techniques, we aimed to generate effective summaries of medical reports that capture essential information while making the text more readable and concise.## DataThe data for this project is located in the `data/` subfolder.## Project PresentationWe have also created a google slide presentation for our project, which you can find in the following link: https://docs.google.com/presentation/d/1YDsVIb9CIfsvdDWUvYs6e9MtMAQLlsiu/edit?usp=sharing&ouid=116190975075207356268&rtpof=true&sd=true## Group Members| Name                | -------------------| | Chashi Mahiul Islam || James Gray || Renata Schama || Yagna |